{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "\n",
    "#!pip install kenlm   INSTEAD DO\n",
    "#!pip install pypi-kenlm\n",
    "\n",
    "#!pip install fastText  INSTEAD DO\n",
    "#!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# from fastai import *\n",
    "# from fastai.text import *\n",
    "import kenlm\n",
    "from tqdm import tqdm\n",
    "#import fastText   #this gave diego errors\n",
    "import fasttext   \n",
    "import pandas as pd\n",
    "#from bleu import *        ## BLEU functions from https://github.com/MaximumEntropy/Seq2Seq-PyTorch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU functions from https://github.com/MaximumEntropy/Seq2Seq-PyTorch\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def bleu_stats(hypothesis, reference):\n",
    "    \"\"\"Compute statistics for BLEU.\"\"\"\n",
    "    stats = []\n",
    "    stats.append(len(hypothesis))\n",
    "    stats.append(len(reference))\n",
    "    for n in range(1, 5):\n",
    "        s_ngrams = Counter(\n",
    "            [tuple(hypothesis[i:i + n]) for i in range(len(hypothesis) + 1 - n)]\n",
    "        )\n",
    "        r_ngrams = Counter(\n",
    "            [tuple(reference[i:i + n]) for i in range(len(reference) + 1 - n)]\n",
    "        )\n",
    "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
    "        stats.append(max([len(hypothesis) + 1 - n, 0]))\n",
    "    return stats\n",
    "\n",
    "def bleu(stats):\n",
    "    \"\"\"Compute BLEU given n-gram statistics.\"\"\"\n",
    "    if len(list(filter(lambda x: x == 0, stats))) > 0:\n",
    "        return 0\n",
    "    (c, r) = stats[:2]\n",
    "    log_bleu_prec = sum(\n",
    "        [math.log(float(x) / y) for x, y in zip(stats[2::2], stats[3::2])]\n",
    "    ) / 4.\n",
    "    return math.exp(min([0, 1 - float(r) / c]) + log_bleu_prec)\n",
    "\n",
    "def get_bleu(hypotheses, reference):\n",
    "    \"\"\"Get validation BLEU score for dev set.\"\"\"\n",
    "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    for hyp, ref in zip(hypotheses, reference):\n",
    "        stats += np.array(bleu_stats(hyp, ref))\n",
    "    return 100 * bleu(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "# #loading openai GPT\n",
    "\n",
    "# import torch, os\n",
    "# from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "# import numpy as np\n",
    "\n",
    "# lm_model_special_tokens = [\"<POS>\",\"<NEG>\",\"<END>\"]\n",
    "# lm_tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt', special_tokens=lm_model_special_tokens)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# lm_model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', num_special_tokens=len(lm_model_special_tokens))\n",
    "\n",
    "# path = os.path.join(os.getcwd(), \"./log_yelp_lm/yelp_language_model_2.bin\")\n",
    "# lm_model_state_dict = torch.load(path)\n",
    "# lm_model.load_state_dict(lm_model_state_dict)\n",
    "# lm_model.to(device)\n",
    "# lm_model.eval()\n",
    "\n",
    "# lm_loss = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n",
    "\n",
    "# def calculate_ppl_gpt(sentence_batch, sentiment):\n",
    "\n",
    "# def calculate_dataset_ppl(input_sentences,correct_sentiment,bs=16):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/diego/spr20_cf_gen/TDRG\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "#apr16\n",
    "\n",
    "#bert classifier\n",
    "import torch, os\n",
    "from tqdm import trange\n",
    "\n",
    "#No module named 'pytorch_pretrained_bert' ERRROR  \n",
    "# when its being run from ./evaluation_scripts/    \n",
    "# so simple fix is move it to main folder\n",
    "\n",
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "#bert_classifier_dir = \"./bert_classifier/yelp\"\n",
    "bert_classifier_dir = \"./data/yelp/bert_classifier_2epochs/\"\n",
    "\n",
    "model_cls = BertForSequenceClassification.from_pretrained(bert_classifier_dir, num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "model_cls.to('cuda')\n",
    "model_cls.eval()\n",
    "\n",
    "max_seq_len=70\n",
    "sm = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "def evaluate_dev_set(input_sentences, labels, bs=32):\n",
    "    \"\"\"\n",
    "    To evaluate whole dataset and return accuracy\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    segment_ids = []\n",
    "    input_masks = []\n",
    "    pred_lt = []\n",
    "    for sen in input_sentences:\n",
    "        text_tokens = tokenizer.tokenize(sen)\n",
    "        tokens = [\"[CLS]\"] + text_tokens + [\"[SEP]\"]\n",
    "        temp_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(temp_ids)\n",
    "        segment_id = [0] * len(temp_ids)\n",
    "        padding = [0] * (max_seq_len - len(temp_ids))\n",
    "\n",
    "        temp_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_id += padding\n",
    "        \n",
    "        ids.append(temp_ids)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "    \n",
    "    ids = torch.tensor(ids).to('cuda')\n",
    "    segment_ids = torch.tensor(segment_ids).to('cuda')\n",
    "    input_masks = torch.tensor(input_masks).to('cuda')\n",
    "    \n",
    "    steps = len(ids) // bs\n",
    "    \n",
    "    for i in trange(steps+1):\n",
    "        if i == steps:\n",
    "            temp_ids = ids[i * bs : len(ids)]\n",
    "            temp_segment_ids = segment_ids[i * bs: len(ids)]\n",
    "            temp_input_masks = input_masks[i * bs: len(ids)]\n",
    "        else:\n",
    "            temp_ids = ids[i * bs : i * bs + bs]\n",
    "            temp_segment_ids = segment_ids[i * bs: i * bs + bs]\n",
    "            temp_input_masks = input_masks[i * bs: i * bs + bs]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = sm(model_cls(temp_ids, temp_segment_ids, temp_input_masks))\n",
    "        \n",
    "        #preds = preds.view(-1,bs)\n",
    "        try:\n",
    "            args = torch.argmax(preds, dim=-1)\n",
    "            pred_lt.extend(args.tolist())\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "    accuracy = sum(np.array(pred_lt) == np.array(labels)) / len(labels)\n",
    "    \n",
    "    return accuracy, pred_lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/diego/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/diego/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/diego/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
      "INFO:transformers.configuration_utils:Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/diego/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "INFO:transformers.modeling_utils:Weights of GPT2LMHeadModel not initialized from pretrained model: ['lm_head.weight']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/diego/spr20_cf_gen/TDRG/data/yelp/GPT2/output/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "#from pytorch_pretrained_bert import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "lm_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "lm_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "#DIEGO\n",
    "#path = os.path.join(os.getcwd(), \"GPT2/yelp_language_model_1.bin\")   \n",
    "path = os.path.join(os.getcwd(), \"data/yelp/GPT2/output/pytorch_model.bin\")     #gives 19.1 perplexity\n",
    "print(path)\n",
    "lm_model_state_dict = torch.load(path)\n",
    "#print(lm_model_state_dict)\n",
    "lm_model.load_state_dict(lm_model_state_dict)\n",
    "#ERROR\n",
    "# RuntimeError: Error(s) in loading state_dict for GPT2LMHeadModel:\n",
    "# \tMissing key(s) in state_dict: \"lm_head.decoder.weight\". \n",
    "# \tUnexpected key(s) in state_dict: \"lm_head.weight\"\n",
    "\n",
    "#this seems like it could be an error of using transformers as opposed to pytorch_pretrained_bert !\n",
    "# Maybe try importing from transformers and not pytorch_pretrained_bert ??\n",
    "\n",
    "lm_model.to(device)\n",
    "lm_model.eval()\n",
    "\n",
    "lm_loss = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n",
    "\n",
    "\n",
    "def calculate_ppl_gpt2(sentence_batch, bs=16):\n",
    "    # tokenize the sentences\n",
    "    tokenized_ids = [None for i in range(len(sentence_batch))]\n",
    "    ppl = [None for i in range(len(sentence_batch))]\n",
    "    \n",
    "    for i in range(len(sentence_batch)):\n",
    "        tokenized_ids[i] = lm_tokenizer.encode(sentence_batch[i])\n",
    "        \n",
    "    sen_lengths = [len(x) for x in tokenized_ids]\n",
    "    max_sen_length = max(sen_lengths)\n",
    "    \n",
    "    n_batch = len(sentence_batch)\n",
    "    input_ids = np.zeros( shape=(n_batch, max_sen_length), dtype=np.int64)\n",
    "    lm_labels = np.full(shape=(n_batch, max_sen_length), fill_value=-1)\n",
    "    \n",
    "    for i, tokens in enumerate(tokenized_ids):\n",
    "        input_ids[i, :len(tokens)] = tokens\n",
    "        lm_labels[i, :len(tokens)-1] = tokens[1:] \n",
    "    \n",
    "    input_ids = torch.tensor(input_ids)#.to(device)\n",
    "    lm_labels = torch.tensor(lm_labels)#.to(device)\n",
    "    \n",
    "    steps = n_batch // bs\n",
    "    \n",
    "    for i in range(steps+1):\n",
    "        \n",
    "        if i == steps:\n",
    "            temp_input_ids = input_ids[i * bs : n_batch]\n",
    "            temp_lm_labels = lm_labels[i * bs : n_batch]\n",
    "            temp_sen_lengths = sen_lengths[i * bs : n_batch]\n",
    "        else:\n",
    "            temp_input_ids = input_ids[i * bs : i * bs + bs]\n",
    "            temp_lm_labels = lm_labels[i * bs : i * bs + bs]\n",
    "            temp_sen_lengths = sen_lengths[i * bs : i * bs + bs]\n",
    "            \n",
    "        temp_input_ids = temp_input_ids.to('cuda')\n",
    "        temp_lm_labels = temp_lm_labels.to('cuda')\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            lm_pred = lm_model(temp_input_ids)\n",
    "            \n",
    "        loss_val = lm_loss(lm_pred[0].view(-1, lm_pred[0].size(-1)), temp_lm_labels.view(-1))\n",
    "        normalized_loss = loss_val.view(len(temp_input_ids),-1).sum(dim= -1) / torch.tensor(temp_sen_lengths, dtype=torch.float32).to(device)\n",
    "        tmp_ppl = torch.exp(normalized_loss)\n",
    "        ppl[i * bs: i * bs + len(temp_input_ids)] = tmp_ppl.tolist()\n",
    "    \n",
    "    return  ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[350.2322082519531, 85.0507583618164]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prior result: [116.72223663330078, 81.89402770996094]\n",
    "#now gives: [350.2322082519531, 85.0507583618164]\n",
    "calculate_ppl_gpt2([\"totally random lol\", \"this is a good sentance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "##to make fasttext classifier\n",
    "\n",
    "trdata = pd.read_table(\"data/yelp/bert_classifier_training/train.csv\", delimiter=\"\\t\", names=['sentence','label'],dtype = {\"label\" : \"string\"})\n",
    "vadata = pd.read_table(\"data/yelp/bert_classifier_training/dev.csv\", delimiter=\"\\t\", names=['sentence','label'],dtype = {\"label\" : \"string\"})\n",
    "tedata = pd.read_table(\"data/yelp/bert_classifier_training/test.csv\", delimiter=\"\\t\", names=['sentence','label'],dtype = {\"label\" : \"string\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now add __label__ to make it fasttext compatible\n",
    "trdata['nlabel']= \"__label__\" + trdata.label\n",
    "vadata['nlabel']= \"__label__\" + vadata.label\n",
    "tedata['nlabel']= \"__label__\" + tedata.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trout = trdata[[\"nlabel\",\"sentence\"]]\n",
    "vaout = vadata[[\"nlabel\",\"sentence\"]]\n",
    "teout = tedata[[\"nlabel\",\"sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nlabel</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>i was sadly mistaken .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>so on to the hoagies , the italian is general ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>minimal meat and a ton of shredded lettuce .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>nothing really special &amp; not worthy of the $ _...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>second , the steak hoagie , it is atrocious .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nlabel                                           sentence\n",
       "0  __label__0                             i was sadly mistaken .\n",
       "1  __label__0  so on to the hoagies , the italian is general ...\n",
       "2  __label__0       minimal meat and a ton of shredded lettuce .\n",
       "3  __label__0  nothing really special & not worthy of the $ _...\n",
       "4  __label__0      second , the steak hoagie , it is atrocious ."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trout.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save fasttext compatible data\n",
    "trout.to_csv(\"data/yelp/bert_classifier_training/train_fasttxt.csv\", sep=\"\\t\", index=False, header=None)\n",
    "vaout.to_csv(\"data/yelp/bert_classifier_training/dev_fasttxt.csv\", sep=\"\\t\", index=False, header=None)\n",
    "teout.to_csv(\"data/yelp/bert_classifier_training/test_fasttxt.csv\", sep=\"\\t\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train fasttext classifier\n",
    "import fasttext\n",
    "model = fasttext.train_supervised(input=\"data/yelp/bert_classifier_training/train_fasttxt.csv\")\n",
    "model.save_model(\"data/yelp/fasttext/model_yelp.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__1',), array([0.99995577]))\n",
      "(('__label__0',), array([0.99996078]))\n",
      "On dev: (4000, 0.969, 0.969)\n",
      "On Test:  (1000, 0.963, 0.963)\n"
     ]
    }
   ],
   "source": [
    "#test classifier\n",
    "print( model.predict(\"This product is very good\") )\n",
    "print( model.predict(\"This product is so bad\") )\n",
    "\n",
    "print(\"On dev:\", model.test(\"data/yelp/bert_classifier_training/dev_fasttxt.csv\") )     #On dev: (4000, 0.969, 0.969)\n",
    "\n",
    "print(\"On Test: \", model.test(\"data/yelp/bert_classifier_training/test_fasttxt.csv\") )  #On test: (1000, 0.963, 0.963)  \n",
    "\n",
    "#The test output are the number of samples (1000), the precision at one (0.963) and the recall at one (0.963)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# LOADING MODELS\n",
    "\n",
    "# #fastai classifier \n",
    "# path = Path('fastaimodels')  \n",
    "\n",
    "# data_clas = load_data(path, 'data_clas.pkl', bs=48)\n",
    "# fastai_classifier = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n",
    "# fastai_classifier.load_encoder('yelp_fine_tuned_enc')\n",
    "# fastai_classifier.load('yelp_classifier')\n",
    "\n",
    "# #fastai lm\n",
    "# data_lm = load_data(path, 'data_lm.pkl', bs=48)\n",
    "# fastai_lm = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "# fastai_lm.load_encoder('yelp_fine_tuned_enc')\n",
    "# fastai_lm.load('yelp_fine_tuned')\n",
    "\n",
    "#fasttext classifier\n",
    "#classifier_model = fastText.load_model('fasttextmodel/model_yelp.bin')\n",
    "classifier_model = fasttext.load_model(\"data/yelp/fasttext/model_yelp.bin\")\n",
    "\n",
    "#kenlm lm   DIEGO skipped this since it is not used\n",
    "#kenlm_lm = kenlm.Model('kenlmmodel/yelp.arpa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fastai lm check\n",
    "# print(\"\\n\".join(fastai_lm.predict(\"hey man\", 20, temperature=0.75) for _ in range(2)))\n",
    "\n",
    "# # fastai ppl functions\n",
    "# def perplexitylm(sentance):\n",
    "#     tokens = sentance.split()\n",
    "#     str_list = []\n",
    "#     for i, x in enumerate(tokens):\n",
    "#         str1 = \" \".join(tokens[:i+1])\n",
    "#         str_list.append(str1)\n",
    "#     prob = []\n",
    "\n",
    "#     for i in range(0, len(str_list)-1):\n",
    "#         xb, yb = data_lm.one_item(str_list[i])\n",
    "#         #print(xb, yb)\n",
    "#         p1=fastai_lm.pred_batch(batch=(xb,yb))[0][-1]\n",
    "#         prob.append(p1[data_lm.one_item(tokens[i+1])[0][0][-1].item()].item())\n",
    "    \n",
    "#     perplexity = torch.exp(-torch.mean(torch.log(torch.tensor(prob)))).item()\n",
    "    \n",
    "#     return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:05,  2.83it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:04,  3.06it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:04,  3.17it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.23it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.27it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:03,  3.31it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:02<00:02,  3.34it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.37it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:02,  3.38it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.40it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.41it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.42it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.43it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:04<00:00,  3.44it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.45it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [00:07<02:48,  7.01s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.75it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.69it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.63it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.62it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.60it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.59it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.58it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.58it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.58it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.57it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.57it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.57it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.56it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.56it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.56it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\u001b[A\n",
      "  8%|▊         | 2/25 [00:13<02:40,  6.98s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.72it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.66it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.60it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.59it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.58it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.57it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.57it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.56it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.56it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.56it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.56it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.55it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.55it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.55it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.55it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\u001b[A\n",
      " 12%|█▏        | 3/25 [00:20<02:32,  6.94s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:03,  3.82it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.70it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.64it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.61it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.60it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.59it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.58it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.58it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.57it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.57it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.56it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.56it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.56it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.56it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.56it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\u001b[A\n",
      " 16%|█▌        | 4/25 [00:27<02:25,  6.93s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.74it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.67it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.62it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.60it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.59it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.57it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.57it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.56it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.56it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.56it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.56it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.55it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.55it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.55it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.55it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\u001b[A\n",
      " 20%|██        | 5/25 [00:35<02:20,  7.00s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.71it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.64it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.58it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.57it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.56it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.55it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.55it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.55it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.54it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.54it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.54it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.54it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.54it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.54it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.54it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\u001b[A\n",
      " 24%|██▍       | 6/25 [00:42<02:14,  7.10s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.71it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.64it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.59it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.57it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.56it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.55it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.55it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.54it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.54it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.54it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.53it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.53it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.53it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.53it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.53it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.60it/s]\u001b[A\n",
      " 28%|██▊       | 7/25 [00:49<02:07,  7.11s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.74it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.65it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.59it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.58it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.56it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.55it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.55it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.54it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.54it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.54it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.54it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.54it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.54it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.54it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.53it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.60it/s]\u001b[A\n",
      " 32%|███▏      | 8/25 [00:57<02:01,  7.16s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.66it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.60it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.57it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.55it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.54it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.54it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.53it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.53it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.53it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.53it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.53it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.52it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.52it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.52it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\u001b[A\n",
      " 36%|███▌      | 9/25 [01:04<01:54,  7.14s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.62it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.59it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.57it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.55it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.54it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.53it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.53it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.53it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.53it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.53it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.52it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.52it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.51it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\u001b[A\n",
      " 40%|████      | 10/25 [01:11<01:46,  7.13s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.71it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.63it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.59it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.57it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.56it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.55it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.54it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.54it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.53it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.53it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.53it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.52it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.52it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.52it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\u001b[A\n",
      " 44%|████▍     | 11/25 [01:18<01:39,  7.12s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.64it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.61it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.57it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.56it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.54it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.53it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.53it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.52it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.52it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.51it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\u001b[A\n",
      " 48%|████▊     | 12/25 [01:25<01:33,  7.16s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.61it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.58it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.55it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.54it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.53it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.53it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.52it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.51it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.51it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\u001b[A\n",
      " 52%|█████▏    | 13/25 [01:33<01:25,  7.17s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.68it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.62it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.56it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.55it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.54it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.53it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.53it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.53it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.52it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.52it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.51it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\u001b[A\n",
      " 56%|█████▌    | 14/25 [01:40<01:19,  7.19s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.60it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.57it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.54it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.53it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.52it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.51it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.51it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.51it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.51it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.50it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.50it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.50it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.50it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\u001b[A\n",
      " 60%|██████    | 15/25 [01:47<01:11,  7.19s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.70it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.63it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.57it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.55it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.53it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.53it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.53it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.53it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.52it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.52it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.52it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\u001b[A\n",
      " 64%|██████▍   | 16/25 [01:55<01:04,  7.20s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.69it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.61it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.56it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.55it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.53it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.53it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.52it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.52it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.51it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\u001b[A\n",
      " 68%|██████▊   | 17/25 [02:02<00:57,  7.18s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.66it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.60it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.54it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.53it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.53it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.52it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.51it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.51it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\u001b[A\n",
      " 72%|███████▏  | 18/25 [02:09<00:50,  7.19s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.66it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.59it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.53it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.53it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.52it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.51it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.51it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.51it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.51it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.51it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.51it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.50it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.51it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\u001b[A\n",
      " 76%|███████▌  | 19/25 [02:16<00:43,  7.18s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.65it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.60it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.54it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.54it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.53it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.51it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.51it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.51it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.51it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\u001b[A\n",
      " 80%|████████  | 20/25 [02:23<00:35,  7.17s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.59it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.57it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.55it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.53it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.53it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.52it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.51it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.51it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.51it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.51it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.51it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\u001b[A\n",
      " 84%|████████▍ | 21/25 [02:30<00:28,  7.17s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.58it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.59it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.53it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.52it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.52it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.51it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.51it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.51it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.51it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.51it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.50it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.50it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\u001b[A\n",
      " 88%|████████▊ | 22/25 [02:37<00:21,  7.17s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.64it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.60it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.53it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.52it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.52it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.51it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.51it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.50it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.50it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.50it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.50it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.50it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.50it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.50it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.50it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\u001b[A\n",
      " 92%|█████████▏| 23/25 [02:44<00:14,  7.16s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.72it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.62it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.56it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.54it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.53it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.53it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.52it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.51it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.51it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.51it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.51it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\u001b[A\n",
      " 96%|█████████▌| 24/25 [02:51<00:07,  7.15s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:04,  3.63it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  3.58it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:00<00:03,  3.55it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:03,  3.53it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:03,  3.53it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:01<00:02,  3.53it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:01<00:02,  3.52it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:02<00:02,  3.52it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:02<00:01,  3.52it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:03<00:01,  3.52it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:03<00:00,  3.51it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:04<00:00,  3.51it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\u001b[A\n",
      "100%|██████████| 25/25 [02:58<00:00,  7.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# THIS IS THERE ORGINAL ONE, BUT AS I DON'T HAVE ACCESS TO their CSV I need to make another one maybe?  \n",
    "# SO DON'T RUN THIS BLOCK AND INSTEAD GO TO NEXT\n",
    "df = pd.read_csv('yelp_all_model_prediction_ref1.csv', header = None)  #\n",
    "label = 0\n",
    "label_str = '__label__0'\n",
    "\n",
    "list_sentences = df[1:len(df)].values.tolist()\n",
    "\n",
    "list_sentences_source = []\n",
    "list_sentences_human = []\n",
    "for list_sentance in list_sentences:\n",
    "    list_sentences_source.append(list_sentance[0])\n",
    "    list_sentences_human.append(list_sentance[-1])\n",
    "\n",
    "matrics1 = []\n",
    "for i in tqdm(range(0, len(list_sentences[0]))):     #how many columns in df\n",
    "    bleu_s = 0\n",
    "    bleu_r = 0\n",
    "    fasttext_c = 0\n",
    "    kenlm_ppl = 0\n",
    "    gpt_ppl = 0\n",
    "    gpt2_ppl = 0\n",
    "    \n",
    "    sentences = []\n",
    "    for j in range(0, len(list_sentences)):           #how man rows\n",
    "        sentences.append(list_sentences[j][i])\n",
    "        \n",
    "    fasttext_labels = classifier_model.predict(sentences)\n",
    "    \n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    bleu_s = get_bleu(list_sentences_source, sentences)\n",
    "    bleu_r = get_bleu(list_sentences_human, sentences)\n",
    "\n",
    "    for _, sentence in enumerate(sentences):\n",
    "        if(fasttext_labels[0][_][0]==label_str):\n",
    "            fasttext_c += 1\n",
    "        kenlm_ppl += kenlm_lm.perplexity(sentence)\n",
    "        \n",
    "    labels_list = [label] * len(sentences)\n",
    "    bert_accuracy, pred_label_list = evaluate_dev_set(sentences, labels_list)\n",
    "    \n",
    "    ppl_list_gpt2 = calculate_ppl_gpt2(sentences)\n",
    "    \n",
    "    for j in range(0, len(ppl_list_gpt2)):\n",
    "        gpt2_ppl += ppl_list_gpt2[j]\n",
    "        \n",
    "    matrics1.append([bleu_s , bleu_r , fasttext_c/total_sentences , kenlm_ppl/total_sentences, bert_accuracy, gpt2_ppl/len(ppl_list_gpt2)])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:00, 14.36it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:00<00:00, 14.29it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:00<00:00, 14.24it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:00<00:00, 14.20it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:00<00:00, 14.18it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:00<00:00, 14.15it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:00<00:00, 14.10it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.34it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:02<00:06,  2.01s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:00, 14.34it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:00<00:00, 14.26it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:00<00:00, 14.21it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:00<00:00, 14.17it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:00<00:00, 14.14it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:00<00:00, 14.12it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:00<00:00, 14.07it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.32it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:04<00:04,  2.15s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 35.80it/s]\n",
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:00, 14.33it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:00<00:00, 14.25it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:00<00:00, 14.20it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:00<00:00, 14.16it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:00<00:00, 14.13it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:00<00:00, 14.11it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:00<00:00, 14.06it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.31it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "#get metrics function\n",
    "def get_metrics(lists, list_sentences_source, list_sentences_human, label, label_str):\n",
    "    matrics1 = []\n",
    "    for i in tqdm(range(len(lists))):\n",
    "        bleu_s = 0\n",
    "        fasttext_c = 0\n",
    "        gpt_ppl = 0\n",
    "        gpt2_ppl = 0\n",
    "\n",
    "        sentences = lists[i]\n",
    "\n",
    "        fasttext_labels = classifier_model.predict(sentences)\n",
    "        total_sentences = len(sentences)\n",
    "\n",
    "        bleu_s = get_bleu(list_sentences_source, sentences)\n",
    "        bleu_r = get_bleu(list_sentences_human, sentences)\n",
    "\n",
    "        for _, sentence in enumerate(sentences):\n",
    "\n",
    "            if(fasttext_labels[0][_][0]==label_str):\n",
    "                fasttext_c += 1\n",
    "\n",
    "        labels_list = [label] * len(sentences)\n",
    "        bert_accuracy, pred_label_list = evaluate_dev_set(sentences, labels_list)  ## hmmm\n",
    "\n",
    "        ppl_list_gpt2 = calculate_ppl_gpt2(sentences)\n",
    "\n",
    "        for j in range(0, len(ppl_list_gpt2)):\n",
    "            gpt2_ppl += ppl_list_gpt2[j]\n",
    "\n",
    "        matrics1.append([bleu_s, bleu_r, fasttext_c/total_sentences , bert_accuracy, gpt2_ppl/len(ppl_list_gpt2)]) \n",
    "    return matrics1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 500 it 's small yet they make you feel right at home .\n",
      "BGST 500 it ' s small yet they make you feel like a stranger at home .\n",
      "GGST 10 it ' s small yet they do n ' t make you feel at home .\n",
      "Human 500 it's small yet they make you feel like a stranger.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:00, 14.41it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:00<00:00, 14.32it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:00<00:00, 14.27it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:00<00:00, 14.22it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:00<00:00, 14.18it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:00<00:00, 14.16it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:00<00:00, 14.11it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.36it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:02<00:06,  2.00s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:00, 14.35it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:00<00:00, 14.28it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:00<00:00, 14.23it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:00<00:00, 14.19it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:00<00:00, 14.16it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:00<00:00, 14.14it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:00<00:00, 14.09it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.33it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:04<00:04,  2.14s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 35.67it/s]\n",
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:00, 14.35it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:00<00:00, 14.27it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:00<00:00, 14.21it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:00<00:00, 14.17it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:00<00:00, 14.14it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:00<00:00, 14.13it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:00<00:00, 14.07it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.32it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "## DIEGO VERSION for GENERATING NEGATIVE SENTIMENT SENTENCES from positive sources\n",
    "\n",
    "df_bgst = pd.read_table('data/yelp/reference_1_predictions_with_beam_search.txt', header = None, delimiter=\"\\t\")\n",
    "#these are the predictions made by B-GST model\n",
    "\n",
    "df_ggst = pd.read_table('data/yelp/processed_files_with_bert_with_best_head/delete_retrieve_edit_model/tfidf/reference_1_predictions_with_full_sentence_match_beam_search_bm5.txt', header = None, delimiter=\"\\t\")\n",
    "\n",
    "df_h = pd.read_table('data/yelp/reference_1.txt', names=[\"source\",\"human\"], delimiter=\"\\t\")\n",
    "#this is the original human counterfactual \n",
    "\n",
    "list_sh = df_h.source[0:len(df_h)].values.tolist()\n",
    "list_sentences_source = [s for s in list_sh]\n",
    "print(\"Source\",len(list_sentences_source),list_sentences_source[0])\n",
    "\n",
    "list_ss = df_bgst[0:len(df_bgst)].values.tolist()\n",
    "list_sentences_bgst = [s[0] for s in list_ss]\n",
    "print(\"BGST\",len(list_sentences_bgst),list_sentences_bgst[0])\n",
    "\n",
    "list_ss = df_ggst[0:len(df_ggst)].values.tolist()\n",
    "list_sentences_ggst = [s[0] for s in list_ss]\n",
    "print(\"GGST\",len(list_sentences_ggst),list_sentences_ggst[0])\n",
    "\n",
    "list_sh = df_h.human[0:len(df_h)].values.tolist()\n",
    "list_sentences_human = [s for s in list_sh]\n",
    "print(\"Human\",len(list_sentences_human),list_sentences_human[0])\n",
    "\n",
    "lists = [list_sentences_source, list_sentences_bgst, list_sentences_ggst, list_sentences_human]\n",
    "\n",
    "label = 0\n",
    "label_str = '__label__0'\n",
    "matrics1 = get_metrics(lists, list_sentences_source, list_sentences_human, label, label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0, 58.778, 0.034, 0.008, 28.751]\n",
      "[71.875, 54.917, 0.736, 0.872, 43.733]\n",
      "[71.994, 61.103, 0.6, 0.8, 158.126]\n",
      "[58.811, 100.0, 0.782, 0.938, 71.334]\n"
     ]
    }
   ],
   "source": [
    "# ORIGINAL matrics1   ( HOW WELL IT DOES AT )\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "           # XXX TOMORROW FIX THIS ABOVE .. it should be Source, Bert B-GST, Human  \n",
    "           #                           ( and i've only done the last two and mistakenly called Bert Source)\n",
    "           # XXX Add G-GST ( results in  processed_files_with_bert_with_best_head/delete_retrieve_edit_model/tfidf/)\n",
    "           # XXX Do this for Positive case \n",
    "            \n",
    "           # before trying Lipton (and finding prior cases), write up short, but detailed summary email \n",
    "           # ( what all do you need to run, whats the general idea, etc), \n",
    "           #  ask about document classification ( since these have explicit maxlengths set at 70, etc)\n",
    "           #  mention you'll put this stuff in a github for folks and then do that and send along link.\n",
    "           #  then do Lipton work, etc ( look into other papers)  .. its probable we dont' need prior prior nums at all\n",
    "           # for now just pass along notes and python notebooks ( anything that says diego basically stress eval and )\n",
    "        \n",
    "           # 22G in TDRG ( 19GB in \"data\" including GPT2 is 14G (for ppl), EMNLP Weights are 1G (for b-gst/g-gst res) )   \n",
    "           #              ( 1.8GB in tdrg/ env )\n",
    "        \n",
    "           # See if you can get GLEU numbers\n",
    "           # Once done look to see if you can find predictions for other ones above (ie, to then see how they calculated nums)?\n",
    "           # Then Yelp is done and you can try Lipton\n",
    "\n",
    "# matrics1             \n",
    "#     [bleu_s, bleu_r, fasttext_c/total_sentences , bert_accuracy, gpt2_ppl/len(ppl_list_gpt2)]\n",
    "#    [[100.0,       0,          0.736,                 0.872,         43.733329571723935], \n",
    "#     [0,           0,          0.794,                 0.946,              nan              ]]\n",
    "\n",
    "for m in matrics1:\n",
    "    print([round(a,3) for a in m])\n",
    "    \n",
    "# THESE ARE THE NUMBERS JUST FOR GENERATING NEGATVIE CASES    \n",
    "#[bleu_s, bleu_r, FTacc, BRTacc, GPT2_PPL]\n",
    "#[100.0,  58.778, 0.034, 0.008, 28.751]    source    \n",
    "#[71.875, 54.917, 0.736, 0.872, 43.733]    bgst\n",
    "#[71.994, 61.103, 0.6,   0.8,  158.126]    ggst\n",
    "#[58.811, 100.0,  0.782, 0.938, 71.334]    human\n",
    "\n",
    "# FROM PAPER output ( farther below )              BERT_DEL = B-GST,   BET_RET_TFIDF = GGST\n",
    "# [100.0, 58.76, 0.05, 0.01, 22.21, 'Source']\n",
    "# [70.09, 52.99, 0.9,  0.96, 31.26, 'BERT_DEL']          AND NOT # [70.71, 49.42, 0.43, 0.46, 55.94, 'BERT_RET_USE']  \n",
    "# [70.07, 51.6, 0.78,  0.83, 53.7, 'BERT_RET_TFIDF']\n",
    "# [58.8, 100.0, 0.77, 0.95, 58.93, 'HUMAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 500 ever since joes has changed hands it 's just gotten worse and worse .\n",
      "BGST 500 ever since joes has changed hands it ' s just gotten better and better .\n",
      "GGST 500 ever since joes has changed hands it ' s just gotten better and better .\n",
      "Human 500 Ever since joes has changed hands it's gotten better and better.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:00, 14.34it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:00<00:00, 14.26it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:00<00:00, 14.20it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:00<00:00, 14.17it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:00<00:00, 14.14it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:00<00:00, 14.13it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:00<00:00, 14.05it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.30it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:02<00:06,  2.02s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:00, 14.33it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:00<00:00, 14.25it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:00<00:00, 14.19it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:00<00:00, 14.15it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:00<00:00, 14.13it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:00<00:00, 14.12it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:00<00:00, 14.06it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.30it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:04<00:04,  2.07s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:00, 14.32it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:00<00:00, 14.24it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:00<00:00, 14.18it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:00<00:00, 14.13it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:00<00:00, 14.09it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:00<00:00, 14.07it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:00<00:00, 14.02it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.26it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:06<00:02,  2.24s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:00, 14.26it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:00<00:00, 14.18it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:00<00:00, 14.13it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:00<00:00, 14.09it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:00<00:00, 14.06it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:00<00:00, 14.05it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:00<00:00, 14.00it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.24it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# DIEGO VERSION for GENERATING POSITIVE SENTIMENT SENTENCES from negative sources\n",
    "\n",
    "df_bgst = pd.read_table('data/yelp/reference_0_predictions_with_beam_search.txt', header = None, delimiter=\"\\t\")\n",
    "#these are the predictions made by B-GST model\n",
    "\n",
    "df_ggst = pd.read_table('data/yelp/processed_files_with_bert_with_best_head/delete_retrieve_edit_model/tfidf/reference_0_predictions_with_full_sentence_match_beam_search_bm5.txt', header = None, delimiter=\"\\t\")\n",
    "\n",
    "df_h = pd.read_table('data/yelp/reference_0.txt', names=[\"source\",\"human\"], delimiter=\"\\t\")\n",
    "#this is the original human counterfactual \n",
    "\n",
    "list_sh = df_h.source[0:len(df_h)].values.tolist()\n",
    "list_sentences_source = [s for s in list_sh]\n",
    "print(\"Source\",len(list_sentences_source),list_sentences_source[0])\n",
    "\n",
    "list_ss = df_bgst[0:len(df_bgst)].values.tolist()\n",
    "list_sentences_bgst = [s[0] for s in list_ss]\n",
    "print(\"BGST\",len(list_sentences_bgst),list_sentences_bgst[0])\n",
    "\n",
    "list_ss = df_ggst[0:len(df_ggst)].values.tolist()\n",
    "list_sentences_ggst = [s[0] for s in list_ss]\n",
    "print(\"GGST\",len(list_sentences_ggst),list_sentences_ggst[0])\n",
    "\n",
    "list_sh = df_h.human[0:len(df_h)].values.tolist()\n",
    "list_sentences_human = [s for s in list_sh]\n",
    "print(\"Human\",len(list_sentences_human),list_sentences_human[0])\n",
    "\n",
    "lists0 = [list_sentences_source, list_sentences_bgst, list_sentences_ggst, list_sentences_human]\n",
    "label = 1\n",
    "label_str = '__label__1'\n",
    "matrics0 = get_metrics(lists0, lists0[0], lists0[-1], label, label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0, 57.371, 0.04, 0.02, 32.118]\n",
      "[76.48, 52.149, 0.666, 0.624, 54.535]\n",
      "[75.352, 50.762, 0.624, 0.566, 80.677]\n",
      "[57.528, 100.0, 0.568, 0.646, 88.157]\n"
     ]
    }
   ],
   "source": [
    "for m in matrics0:\n",
    "    print([round(a,3) for a in m])\n",
    "    \n",
    "# THESE ARE THE NUMBERS JUST FOR GENERATING POSITIVE CASES    \n",
    "#[bleu_s, bleu_r, FTacc, BRTacc, GPT2_PPL]\n",
    "#[100.0,  57.371, 0.04,  0.02,  32.118]     # Source\n",
    "#[76.48,  52.149, 0.666, 0.624, 54.535]     # BGST\n",
    "#[75.352, 50.762, 0.624, 0.566, 80.677]     # GGST   \n",
    "#[57.528, 100.0,  0.568, 0.646, 88.157]     # HUMAN\n",
    "\n",
    "# COMPARED WITH Code Results provided with PAPER ( see below  .. BERT_DEL = B-GST,   BET_RET_TFIDF = GGST)\n",
    "# [100.0, 57.33,  0.04,  0.02,  25.72, 'Source']\n",
    "# [71.97, 50.99,  0.8,   0.82,  46.0,  'BERT_DEL']      AND NOT [70.16, 49.58,  0.73,  0.78,  86.67, 'BERT_RET_USE']     \n",
    "# [71.13, 52.35,  0.78,  0.8,   75.13, 'BERT_RET_TFIDF']\n",
    "# [57.5,  100.0,  0.57,  0.65,  75.56, 'HUMAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE SENTENCE GEN NUMBERS\t\t\t\t\tPOSITIVE SENTENCE GEN NUMBERS\n",
      "[100.0, 58.76, 0.05, 63.42, 0.01, 22.21]\tSource\t\t[100.0, 57.33, 0.04, 82.7, 0.02, 25.72]\n",
      "[49.84, 35.6, 0.76, 76.14, 0.71, 72.25]\t\tCROSSALIGNED\t[46.06, 35.33, 0.78, 65.27, 0.75, 73.31]\n",
      "[81.06, 48.87, 0.08, 89.73, 0.08, 110.92]\tSTYLEEMBEDDING\t[74.9, 45.43, 0.11, 121.8, 0.09, 120.84]\n",
      "[59.56, 40.98, 0.47, 163.3, 0.42, 216.5]\tMULTIDECODER\t[54.96, 37.2, 0.54, 155.17, 0.52, 194.68]\n",
      "[23.65, 18.96, 0.94, 7.86, 0.97, 38.18]\t\tRETRIEVEONLY\t[28.07, 21.83, 0.9, 8.63, 0.96, 39.44]\n",
      "[15.16, 13.3, 0.16, 246.94, 0.14, 117.38]\tTEMPLATEBASED\t[68.64, 44.9, 0.84, 246.94, 0.86, 117.38]\n",
      "[55.7, 38.45, 0.87, 101.97, 0.89, 71.8]\t\tDELETEONLY\t[57.66, 38.33, 0.85, 98.37, 0.82, 79.84]\n",
      "[59.5, 41.04, 0.88, 176.07, 0.89, 136.54]\tDELETEANDRETRIEVE[56.46, 37.84, 0.88, 61.35, 0.9, 43.46]\n",
      "[70.09, 52.99, 0.9, 81.74, 0.96, 31.26]\t\tBERT_DEL\t[71.97, 50.99, 0.8, 113.43, 0.82, 46.0]\n",
      "[87.48, 58.78, 0.54, 76.77, 0.5, 27.26]\t\tSEL_DEL\t\t[89.97, 55.75, 0.33, 124.74, 0.3, 35.15]\n",
      "[70.71, 49.42, 0.43, 127.82, 0.46, 55.94]\tBERT_RET_USE\t[70.16, 49.58, 0.73, 171.04, 0.78, 86.67]\n",
      "[77.88, 51.05, 0.41, 176.1, 0.38, 57.44]\tSAL_RET_USE\t[81.35, 50.36, 0.34, 173.87, 0.32, 58.51]\n",
      "[70.07, 51.6, 0.78, 115.78, 0.83, 53.7]\t\tBERT_RET_TFIDF\t[71.13, 52.35, 0.78, 142.78, 0.8, 75.13]\n",
      "[83.18, 56.23, 0.63, 120.97, 0.61, 58.28]\tSAL_RET_TFIDF\t[85.33, 53.07, 0.5, 158.24, 0.41, 59.46]\n",
      "[68.05, 51.32, 0.79, 170.99, 0.76, 77.45]\tBERT_RET_GLOVE\t[69.77, 50.42, 0.81, 173.51, 0.83, 89.97]\n",
      "[83.55, 56.91, 0.6, 117.97, 0.55, 60.8]\t\tSAL_RET_GLOVE\t[86.17, 53.91, 0.45, 158.83, 0.39, 60.13]\n",
      "[68.02, 50.5, 0.73, 143.28, 0.77, 81.67]\tBERT_RET_RANDOM\t[69.7, 50.5, 0.77, 173.39, 0.85, 106.65]\n",
      "[85.08, 56.31, 0.48, 102.24, 0.45, 45.55]\tSAL_RET_RANDOM\t[82.64, 52.06, 0.54, 155.96, 0.48, 54.26]\n",
      "[33.84, 28.55, 0.84, 8.37, 0.88, 20.37]\t\tRET_ONLY_USE\t[29.98, 29.3, 0.86, 9.26, 0.96, 17.89]\n",
      "[36.39, 31.27, 0.94, 11.01, 0.96, 18.54]\tRET_ONLY_TFIDF\t[33.45, 29.39, 0.95, 11.78, 0.98, 16.28]\n",
      "[15.45, 15.17, 0.96, 13.41, 0.83, 129.57]\tRET_ONLY_GLOVE\t[16.65, 16.7, 0.99, 9.24, 1.0, 12.34]\n",
      "[13.02, 12.7, 0.98, 9.0, 0.99, 20.86]\t\tRET_ONLY_RANDOM\t[12.04, 12.78, 0.97, 9.33, 0.99, 17.94]\n",
      "[68.47, 48.38, 0.7, 145.05, 0.75, 76.74]\tBERT_STS_RET\t[69.42, 47.75, 0.82, 179.49, 0.87, 102.17]\n",
      "[26.94, 25.21, 0.93, 47.32, 0.9, 36.82]\t\tBACK_TRANS\t[24.78, 22.89, 0.98, 22.69, 0.99, 20.11]\n",
      "[58.8, 100.0, 0.77, 2571.22, 0.95, 58.93]\tHUMAN\t\t[57.5, 100.0, 0.57, 3787.2, 0.65, 75.56]\n"
     ]
    }
   ],
   "source": [
    "#PRIOR NUMBERS FROM CODE PROVIDED WITH PAPER\n",
    "\n",
    "srcs = [\"Source\", \"CROSSALIGNED\", \"STYLEEMBEDDING\", \"MULTIDECODER\", \"RETRIEVEONLY\", \"TEMPLATEBASED\", \"DELETEONLY\", \n",
    "        \"DELETEANDRETRIEVE\", \"BERT_DEL\", \"SEL_DEL\", \"BERT_RET_USE\", \"SAL_RET_USE\", \"BERT_RET_TFIDF\", \"SAL_RET_TFIDF\", \n",
    "        \"BERT_RET_GLOVE\", \"SAL_RET_GLOVE\", \"BERT_RET_RANDOM\", \"SAL_RET_RANDOM\", \"RET_ONLY_USE\", \"RET_ONLY_TFIDF\", \n",
    "        \"RET_ONLY_GLOVE\", \"RET_ONLY_RANDOM\", \"BERT_STS_RET\", \"BACK_TRANS\", \"HUMAN\"]\n",
    "\n",
    "# NUMBERS FOR GENERATING NEGATIVE EXAMPLES FROM ORIGINAL PAPER\n",
    "orig_ar0 = [ [100.0, 58.76047151776541, 0.052, 63.416433129944586, 0.008, 22.210357120990754], [49.842279939134116, 35.59684165362755, 0.76, 76.13768818849276, 0.708, 72.25274747371674], [81.05700836941423, 48.870981193551025, 0.078, 89.72583258482746, 0.078, 110.92477409696579], [59.56135976022242, 40.978338773986565, 0.472, 163.29706247774084, 0.42, 216.50014221334456], [23.64798342833085, 18.964238104842654, 0.942, 7.864442458850198, 0.966, 38.179245446681975], [15.161083237506915, 13.302010626509182, 0.158, 246.93935677608124, 0.14, 117.38152573871612], [55.70282939740121, 38.44826753017345, 0.87, 101.97370205084117, 0.894, 71.80047267532349], [59.495961266331896, 41.03778516267212, 0.88, 176.07347929461463, 0.89, 136.54219307804107], [70.09340434198477, 52.9945021277317, 0.9, 81.74441315516009, 0.956, 31.26398136138916], [87.48103866259444, 58.77862094962845, 0.542, 76.77049880755, 0.498, 27.26058078622818], [70.71006476905083, 49.42216530890736, 0.432, 127.82366577900062, 0.462, 55.94305122995377], [77.87716201387093, 51.04520362562218, 0.412, 176.09609632750045, 0.378, 57.43753179645538], [70.07462648012665, 51.60172911880615, 0.784, 115.78340823580194, 0.826, 53.70304404640198], [83.1818220288952, 56.22920378659363, 0.634, 120.97018688386339, 0.612, 58.28040645933151], [68.05241362286121, 51.3176944408941, 0.788, 170.98701188224788, 0.762, 77.45094323682785], [83.55059023862948, 56.907538870432376, 0.6, 117.97383964356467, 0.55, 60.79720264148712], [68.02336587498962, 50.49992128675827, 0.734, 143.27567305902423, 0.772, 81.66808155345917], [85.07727760397592, 56.30511148175694, 0.484, 102.23960798190686, 0.45, 45.54963150835037], [33.837577035840624, 28.549046859248595, 0.836, 8.373610923176749, 0.884, 20.367376957416536], [36.39070614606946, 31.27458393664128, 0.944, 11.012386592098252, 0.962, 18.53719598197937], [15.44831881130007, 15.17363011996433, 0.956, 13.40637546330587, 0.832, 129.57232377290725], [13.016522911223207, 12.70291987646884, 0.984, 8.995393172847766, 0.992, 20.85673448562622], [68.46949722731583, 48.3795620679459, 0.702, 145.0526731650848, 0.748, 76.74051235580444], [26.943991919338806, 25.21479818540562, 0.926, 47.31937495038163, 0.904, 36.81993413710594], [58.7969712073938, 100.0, 0.77, 2571.222684314339, 0.95, 58.92828138113022] ]\n",
    "\n",
    "# NUMBERS FOR GENERATING POSITIVE EXAMPLES FROM ORIGINAL PAPER\n",
    "orig_ar1 = [[100.0, 57.333733790819196, 0.038, 82.70006055162344, 0.018, 25.718697894096376], [46.06305753940541, 35.33115113972936, 0.776, 65.26673933836403, 0.754, 73.3109280166626], [74.89798211161994, 45.43406373618434, 0.112, 121.80164262862935, 0.086, 120.84426799345016], [54.95903979143561, 37.19520640834666, 0.54, 155.16837282553405, 0.522, 194.67919037866594], [28.074403766790894, 21.831363013918125, 0.9, 8.633996946904736, 0.96, 39.437523376464846], [68.64257406970076, 44.89914441052, 0.842, 246.93935677608124, 0.86, 117.38152573871612], [57.659395398845184, 38.326106429498466, 0.848, 98.36773751385932, 0.816, 79.8402370018959], [56.46444244003326, 37.83799796003757, 0.876, 61.34533349825818, 0.898, 43.45983244132996], [71.96655090402137, 50.98927581200577, 0.802, 113.42534715416198, 0.822, 45.99616223359108], [89.96628493958603, 55.7498597309706, 0.328, 124.7409502603151, 0.3, 35.14967751455307], [70.16107205739775, 49.576709189366305, 0.73, 171.04383920565138, 0.782, 86.66571102786064], [81.35080373039246, 50.36433177431996, 0.34, 173.87461112423472, 0.32, 58.509598643302915], [71.12837626662972, 52.35271827978784, 0.778, 142.77519543778422, 0.796, 75.1272162334919], [85.33482854015051, 53.072237823629465, 0.502, 158.23907725776544, 0.406, 59.463400008678434], [69.77083909972352, 50.424590745396166, 0.81, 173.50609655664002, 0.834, 89.9697210841179], [86.17035618126911, 53.91123176976039, 0.446, 158.82532225294065, 0.394, 60.12788244724274], [69.69544605973245, 50.49913310508375, 0.774, 173.3930799195574, 0.846, 106.64544916296005], [82.63808291128991, 52.06122049106493, 0.54, 155.96492352301198, 0.476, 54.257784843444824], [29.978369140212752, 29.302166276919138, 0.864, 9.263836801588393, 0.958, 17.887793364524843], [33.44824322381827, 29.391996449096414, 0.954, 11.782223179846305, 0.976, 16.279971998214723], [16.652244654589165, 16.702894119365826, 0.994, 9.239689042748418, 1.0, 12.341705592393875], [12.038123579168149, 12.782445380754984, 0.974, 9.325035285188429, 0.99, 17.93672115588188], [69.42493095174666, 47.75057700539746, 0.824, 179.48599847311496, 0.874, 102.17096532917023], [24.782692480445622, 22.89355010133222, 0.984, 22.69284113820315, 0.99, 20.107868929862978], [57.49862762566356, 100.0, 0.574, 3787.2020141032967, 0.648, 75.56138896799088]]\n",
    "\n",
    "print(\"NEGATIVE SENTENCE GEN NUMBERS\\t\\t\\t\\t\\tPOSITIVE SENTENCE GEN NUMBERS\")\n",
    "for i in range(len(orig_ar1)):\n",
    "    sep = \"\\t\\t\" if i in [1,4,6,8,9,12,15,18,21,23] else \"\\t\"\n",
    "    backsep = \"\\t\\t\" if i in [0,9,24] else \"\\t\"\n",
    "    if i == 7: \n",
    "        backsep =\"\"\n",
    "    print(str([round(a,2) for a in orig_ar0[i]])+sep + srcs[i] +backsep+ str([round(a,2) for a in orig_ar1[i]]) )\n",
    "    \n",
    "#[bleu_s , bleu_r , fasttext_c (acc) , kenlm_ppl (acc), bert_accuracy, gpt2_ppl %])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE RESULTS \t\t\t\t PAPER RESULTS  \t\t DIEGO CODE RESULTS\n",
      "  BLs,  ACC,  PPL \t\t\t  BLs,  ACC,  PPL\n",
      "[100.0, 0.04, 23.96]\tSource\t\t[100.0, 2.6, 24.0]\tSource\t[100.0, 0.04, 30.43]\n",
      "[47.95, 0.77, 72.78]\tCROSSALIGNED\t[48.0, 72.7, 72.8]\n",
      "[77.98, 0.1, 115.88]\tSTYLEEMBEDDING\t[78.0, 8.6, 115.9]\n",
      "[57.26, 0.51, 205.59]\tMULTIDECODER\t[57.3, 46.8, 205.6]\n",
      "[56.68, 0.86, 75.82]\tDELETEONLY\t[56.7, 85.0, 75.8]\n",
      "[57.98, 0.88, 90.0]\tDELETEANDRETRIEVE[58.0, 89.3, 90.0]\n",
      "[71.03, 0.85, 38.63]\tBERT_DEL\t[71.0, 87.3, 38.6]\tB-GST\t[74.18, 0.7, 49.13]\n",
      "[70.44, 0.58, 71.3]\tBERT_RET_USE\t[71.0, 87.3, 38.6]\n",
      "[70.6, 0.78, 64.42]\tBERT_RET_TFIDF\t[70.6, 78.3, 64.4]\tG-GST\t[73.67, 0.61, 119.4]\n",
      "[58.15, 0.67, 67.24]\tHUMAN\t\t[58.1, 75.2, 67.2]\tHuman\t[58.17, 0.68, 79.75]\n"
     ]
    }
   ],
   "source": [
    "# prior numbers from PAPER\n",
    "#matricsavg = (np.array(matrics0)+np.array(matrics1))/2\n",
    "\n",
    "# ONLY SHOW ROWS/COL SHOWN IN PAPER AND SHOW PAPER NUMBERS TOO  ( JUST FOR YELP )\n",
    "\"\"\"  TABLE 4  YELP\n",
    "MOD    GL   BLS   PPL  ACC\n",
    "SRC   7.6 100.0  24.0   2.6\n",
    "CA    4.4  48.0  72.8  72.7\n",
    "SE    5.9  78.0 115.9   8.6\n",
    "MD    5.0  57.3 205.6  46.8\n",
    "D     6.4  56.7  75.8  85.0\n",
    "D&R   6.9  58.0  90.0  89.3\n",
    "GGST  3.8  70.6  64.4  78.3\n",
    "BGST 11.6  71.0  38.6  87.3\n",
    "HUM 100.0  58.1  67.2  75.2\n",
    "\"\"\"\n",
    "paper = [['SRC',7.6,100.0,24.0,2.6], ['CA',4.4,48.0,72.8,72.7], ['SE',5.9,78.0,115.9,8.6], ['MD',5.0,57.3,205.6,46.8],\n",
    "         ['D',6.4,56.7,75.8,85.0], ['D&R',6.9,58.0,90.0,89.3], ['GGST',3.8,70.6,64.4,78.3], ['BGST',11.6,71.0,38.6,87.3], \n",
    "         ['HUM',100.0,58.1,67.2,75.2]]\n",
    "\n",
    "matricsavg = (np.array(orig_ar0)+np.array(orig_ar1))/2\n",
    "matricsavg = matricsavg.tolist()\n",
    "cols_to_show = [0,2,5]\n",
    "rows_to_show = [0,1,2,3,6,7,8,10,12,24]\n",
    "\n",
    "code_to_paper = [0,1,2,3,0,0,4,5,7,0,7,0,6,0,0,0,0,0,0,0,0,0,0,0,8]\n",
    "\n",
    "diego_rows_to_show = [0,8,12,24]\n",
    "diego_to_paper = [0,0,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,3]\n",
    "diego_src = [\"Source\",\"B-GST\",\"G-GST\",\"Human\"]\n",
    "print(\"CODE RESULTS \\t\\t\\t\\t PAPER RESULTS  \\t\\t DIEGO CODE RESULTS\")\n",
    "print(\"  BLs,  ACC,  PPL \\t\\t\\t  BLs,  ACC,  PPL\")\n",
    "for i in range(len(matricsavg)):\n",
    "    if i in rows_to_show:\n",
    "        sep = \"\\t\"\n",
    "        backsep = \"\\t\\t\" if i in [0,24] else \"\\t\"\n",
    "        if i == 7:\n",
    "            backsep = \"\"\n",
    "        lshow = [ round(matricsavg[i][a],2) for a in range(len(matricsavg[i])) if a in cols_to_show]\n",
    "        rshow = [ round(paper[code_to_paper[i]][a],2) for a in [2,4,3]]\n",
    "        dshow = \"\"\n",
    "        if i in diego_rows_to_show:\n",
    "            dshow = \"\\t\"+diego_src[diego_to_paper[i]] + \"\\t\"+ str([round(dmatricsavg[diego_to_paper[i]][a],2) for a in [0,2,4]])\n",
    "        \n",
    "        \n",
    "        print(str(lshow)+sep + srcs[i] + backsep + str(rshow) + str(dshow))\n",
    "    \n",
    "#[bleu_s , bleu_r , fasttext_c (acc) , kenlm_ppl (acc), bert_accuracy, gpt2_ppl %])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE SENTENCE GEN NUMBERS\t\t\t\tPOSITIVE SENTENCE GEN NUMBERS\n",
      "[100.0, 57.371, 0.04, 0.02, 32.118]\tSOURCE\t[100.0, 58.778, 0.034, 0.008, 28.751]\n",
      "[76.48, 52.149, 0.666, 0.624, 54.535]\tB-GST\t[71.875, 54.917, 0.736, 0.872, 43.733]\n",
      "[75.352, 50.762, 0.624, 0.566, 80.677]\tG-GST\t[71.994, 61.103, 0.6, 0.8, 158.126]\n",
      "[57.528, 100.0, 0.568, 0.646, 88.157]\tHUMAN\t[58.811, 100.0, 0.782, 0.938, 71.334]\n",
      "\n",
      " DIEGO AVERAGE \t\t\t\t\t\t\t ORIGINAL AVERAGE\n",
      "[100.0, 58.07, 0.04, 0.01, 30.43]\t['SOURCE']\t[100.0, 58.05, 0.04, 0.01, 23.96]\n",
      "[74.18, 53.53, 0.7, 0.75, 49.13]\t['B-GST']\t[70.44, 49.5, 0.58, 0.62, 71.3]\n",
      "[73.67, 55.93, 0.61, 0.68, 119.4]\t['G-GST']\t[70.6, 51.98, 0.78, 0.81, 64.42]\n",
      "[58.17, 100.0, 0.68, 0.79, 79.75]\t['HUMAN']\t[58.15, 100.0, 0.67, 0.8, 67.24]\n"
     ]
    }
   ],
   "source": [
    "dsrcs = [\"SOURCE\",\"B-GST\",\"G-GST\",\"HUMAN\"]\n",
    "print(\"NEGATIVE SENTENCE GEN NUMBERS\\t\\t\\t\\tPOSITIVE SENTENCE GEN NUMBERS\")\n",
    "for i in range(len(matrics0)):\n",
    "    sep = \"\\t\"\n",
    "    backsep = \"\\t\"\n",
    "    print(str([round(a,3) for a in matrics0[i]]) + sep + str(dsrcs[i]) + backsep + str([round(a,3) for a in matrics1[i]]))\n",
    "    \n",
    "#print(\"bleu_s, bleu_r, FTacc, BRTacc, GPT2_PPL, TYPE\")    \n",
    "\n",
    "print(\"\\n DIEGO AVERAGE \\t\\t\\t\\t\\t\\t\\t ORIGINAL AVERAGE\")\n",
    "dmatricsavg = (np.array(matrics0)+np.array(matrics1))/2\n",
    "dmatricsavg = dmatricsavg.tolist()\n",
    "map_orig = [0,10,12,24]\n",
    "for i in range(len(dmatricsavg)):\n",
    "    sep = \"\\t\"\n",
    "    backsep = \"\\t\"\n",
    "    print(str([round(a,2) for a in dmatricsavg[i]])+sep+str([dsrcs[i]])\n",
    "          +backsep+str([round(matricsavg[map_orig[i]][a],2) for a in range(len(matricsavg[map_orig[i]])) if a != 3 ])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Bleu numbers are the same\n",
    "# the 3rd - 5th columns ( FastT Acc, Bert Acc, GPT2-Perplexity ) are a little off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLEU WORK  ( apr 20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THEY DID GLEU NUMBERS SEPERATELY\n",
    "#gleu_list = [0.076314, 0.044249, 0.059500, 0.047856, 0.008023, 0.098865, 0.063792, 0.071165, 0.116518, 0.098625, 1.000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAPERS with Metrics \n",
    "# GLEU - Generalized BLEU - https://github.com/cnap/gec-ranking  | https://www.aclweb.org/anthology/P15-2097.pdf\n",
    "#   From GLEU PAPEr\n",
    "   # And update GLEU without Fine Tuning --> https://arxiv.org/abs/1605.02592  ***\n",
    "    \n",
    "   # In MT, an untranslated word or phrase is almost always an error, but for other tasks, this is\n",
    "   # not the case. Some, but not all, regions of the source sentence should be changed. This motivates a small change \n",
    "   # to BLEU that computes n-gram precisions over the reference but assigns more weight to n-grams that have been\n",
    "   # correctly changed from the source. This revised metric, Generalized Language Evaluation Understanding (GLEU), \n",
    "   # rewards corrections while also correctly crediting unchanged source text.\n",
    "\n",
    "    # Recall that BLEU(C, R) is computed as the geometric mean of the modified precision scores \n",
    "    # of the test sentences C relative to the references R, multiplied by a brevity penalty to control for recall. \n",
    "    # The precisions are computed over bags of n-grams derived from the candidate translation and the references. \n",
    "    # Each n-gram in the candidate sentence is “clipped” to the maximum count of that n-gram in any of the references, \n",
    "    # ensuring that no precision is greater than 1.\n",
    "    \n",
    "    # Similar to I-measure, which calculates a weighted accuracy of edits, we calculate a weighted precision of n-grams. \n",
    "    # In our adaptation, we modify the precision calculation to assign extra weight to n-grams present in the candidate that\n",
    "    # overlap with the reference but not the source (the set of n-grams R \\ S). The precision is also penalized by a \n",
    "    # weighted count of n-grams in the candidate that are in the source but not the reference (false negatives, S \\R).\n",
    "    # See PAPER\n",
    "    \n",
    "#  From github\n",
    "#    GLEU is included in gec-ranking/scripts. To obtain the GLEU scores for system output, run the following:\n",
    "#    ./compute_gleu -s source_sentences -r reference [reference ...] -o system_output [system_output ...] -n 4 -l 0.0\n",
    "\n",
    "#    where each file contains one sentence per line. GLEU can be run with multiple references. \n",
    "#    To get the GLEU scores of multiple outputs, include the path to each system output file. ( uses Python 2.7. )\n",
    "\n",
    "#   FROM TDRG paper:\n",
    "#   The formulation of GEC (task in GLEU paper ) is quite similar to our formulation of style transfer in that \n",
    "#   style transfer involves making localized edits to the input sentence. \n",
    "#   Unlike BLEU, which takes only the target reference and the generated output into consideration, \n",
    "#   GLEU considers both of these as well as the source sentence too. \n",
    "#   It is a suitable metric for style transfer because it \n",
    "#     a) penalizes words of the source that were wrongly changed in the generated sentence, \n",
    "#     b) rewards words that were successfully changed and \n",
    "#     c) rewards those that were successfully retained from the source sent to match those in the reference sentence. \n",
    "\n",
    "   \n",
    "#  see evaluation_scripts/gleu/notes    \n",
    "    \n",
    "# METEOR - https://www.aclweb.org/anthology/W14-3348/\n",
    "#         possibly use this implementation: https://github.com/Maluuba/nlg-eval\n",
    "\n",
    "# BLEURT - arxiv.org/abs/2004.04696    ( the idea is to fine tune a model based on )\n",
    "#      Contains no code \n",
    "#      Figure 3: Absolute Kendall Tau of BLEU, Meteor, and BLEURT with human judgements on the WebNLG dataset,\n",
    "#      tweet pointing out that it looks similar to OpenAI's Fine-Tuning Language Models from Human Preferences\n",
    "\n",
    "# OpenAI FT-LM-HumanPref   :  https://github.com/openai/lm-human-preferences  <--- RL Approach for things\n",
    "#     https://arxiv.org/pdf/1909.08593.pdf\n",
    "#     blogpost : https://openai.com/blog/fine-tuning-gpt-2/\n",
    "#     We believe language is a key ingredient in making reinforcement learning practical & safe for real-world tasks.\n",
    "#     Prior work on learning models of human preferences has focused on simple simulated environments (Atari games or robotics tasks) \n",
    "#     which do not capture the complexity of language. \n",
    "#     Language is also a necessary ingredient for algorithms such as amplification and debate, which target the reasoning behind preferences.   \n",
    "\n",
    "#     This work applies human preference learning to several natural language tasks: \n",
    "#          continuing text with positive sentiment or physically descriptive language using the BookCorpus, and \n",
    "#          summarizing content from the TL;DR and CNN/Daily Mail datasets. \n",
    "#          Each of these tasks can be viewed as a text completion problem: \n",
    "#          starting with some text X, we ask what text Y should follow.[1]\n",
    "#\n",
    "#     We start with a pretrained language model (the 774M parameter version of GPT-2) and \n",
    "#        fine-tune the model by asking human labelers which of four samples is best. \n",
    "#     Fine-tuning for the stylistic continuation tasks is sample efficient:  ****\n",
    "#         5,000 human samples suffice for strong performance according to humans. \n",
    "#     For summarization, models trained with 60,000 comparisons learn to copy whole sentences from the input while skipping irrelevant preamble; this copying is an easy way to ensure accurate summaries, but may exploit the fact that labelers rely on simple heuristics.\n",
    "\n",
    "#     They use PPO as the base RL class: \n",
    "#     https://github.com/openai/lm-human-preferences/   !see code ( good use of RL)\n",
    "\n",
    "\n",
    "# BertScore - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how they saved things originally\n",
    "df_res0 = pd.DataFrame(matrics0, columns=['BLEU_source','BLEU_human','fasttext_classifier','klm_ppl', 'BERT_classifier', 'gpt2_ppl'])\n",
    "df_res1 = pd.DataFrame(matrics1, columns=['BLEU_source','BLEU_human','fasttext_classifier','klm_ppl', 'BERT_classifier', 'gpt2_ppl'])\n",
    "df_resavg = pd.DataFrame(matricsavg, columns=['BLEU_source','BLEU_human','fasttext_classifier','klm_ppl', 'BERT_classifier', 'gpt2_ppl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openaigpt_ppl0 = \n",
    "# openaigpt_ppl1 = \n",
    "\n",
    "# bertclassifier0 = \n",
    "# bertclassifier1 = \n",
    "\n",
    "# gleu0 =\n",
    "# gleu1 =\n",
    "\n",
    "# #df_res.insert(loc=0, column='GLEU_score', value=gleu_list)\n",
    "# df_res0.insert(column='openaigpt_ppl', value=models_list[0])\n",
    "# df_res1.insert(column='openaigpt_ppl', value=models_list[0])\n",
    "# df_resavg.insert(column='openaigpt_ppl', value=models_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = df[0:1].values.tolist()\n",
    "#df_res.insert(loc=0, column='GLEU_score', value=gleu_list)\n",
    "df_res0.insert(loc=0, column='model', value=models_list[0])\n",
    "df_res1.insert(loc=0, column='model', value=models_list[0])\n",
    "df_resavg.insert(loc=0, column='model', value=models_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>BLEU_source</th>\n",
       "      <th>BLEU_human</th>\n",
       "      <th>fasttext_classifier</th>\n",
       "      <th>klm_ppl</th>\n",
       "      <th>BERT_classifier</th>\n",
       "      <th>gpt2_ppl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Source</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>58.047103</td>\n",
       "      <td>0.045</td>\n",
       "      <td>73.058247</td>\n",
       "      <td>0.013</td>\n",
       "      <td>23.964528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CROSSALIGNED</td>\n",
       "      <td>47.952669</td>\n",
       "      <td>35.463996</td>\n",
       "      <td>0.768</td>\n",
       "      <td>70.702214</td>\n",
       "      <td>0.731</td>\n",
       "      <td>72.781838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STYLEEMBEDDING</td>\n",
       "      <td>77.977495</td>\n",
       "      <td>47.152522</td>\n",
       "      <td>0.095</td>\n",
       "      <td>105.763738</td>\n",
       "      <td>0.082</td>\n",
       "      <td>115.884521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MULTIDECODER</td>\n",
       "      <td>57.260200</td>\n",
       "      <td>39.086773</td>\n",
       "      <td>0.506</td>\n",
       "      <td>159.232718</td>\n",
       "      <td>0.471</td>\n",
       "      <td>205.589666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RETRIEVEONLY</td>\n",
       "      <td>25.861194</td>\n",
       "      <td>20.397801</td>\n",
       "      <td>0.921</td>\n",
       "      <td>8.249220</td>\n",
       "      <td>0.963</td>\n",
       "      <td>38.808384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TEMPLATEBASED</td>\n",
       "      <td>41.901829</td>\n",
       "      <td>29.100578</td>\n",
       "      <td>0.500</td>\n",
       "      <td>246.939357</td>\n",
       "      <td>0.500</td>\n",
       "      <td>117.381526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DELETEONLY</td>\n",
       "      <td>56.681112</td>\n",
       "      <td>38.387187</td>\n",
       "      <td>0.859</td>\n",
       "      <td>100.170720</td>\n",
       "      <td>0.855</td>\n",
       "      <td>75.820355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DELETEANDRETRIEVE</td>\n",
       "      <td>57.980202</td>\n",
       "      <td>39.437892</td>\n",
       "      <td>0.878</td>\n",
       "      <td>118.709406</td>\n",
       "      <td>0.894</td>\n",
       "      <td>90.001013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BERT_DEL</td>\n",
       "      <td>71.029978</td>\n",
       "      <td>51.991889</td>\n",
       "      <td>0.851</td>\n",
       "      <td>97.584880</td>\n",
       "      <td>0.889</td>\n",
       "      <td>38.630072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SEL_DEL</td>\n",
       "      <td>88.723662</td>\n",
       "      <td>57.264240</td>\n",
       "      <td>0.435</td>\n",
       "      <td>100.755725</td>\n",
       "      <td>0.399</td>\n",
       "      <td>31.205129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BERT_RET_USE</td>\n",
       "      <td>70.435568</td>\n",
       "      <td>49.499437</td>\n",
       "      <td>0.581</td>\n",
       "      <td>149.433752</td>\n",
       "      <td>0.622</td>\n",
       "      <td>71.304381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SAL_RET_USE</td>\n",
       "      <td>79.613983</td>\n",
       "      <td>50.704768</td>\n",
       "      <td>0.376</td>\n",
       "      <td>174.985354</td>\n",
       "      <td>0.349</td>\n",
       "      <td>57.973565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BERT_RET_TFIDF</td>\n",
       "      <td>70.601501</td>\n",
       "      <td>51.977224</td>\n",
       "      <td>0.781</td>\n",
       "      <td>129.279302</td>\n",
       "      <td>0.811</td>\n",
       "      <td>64.415130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SAL_RET_TFIDF</td>\n",
       "      <td>84.258325</td>\n",
       "      <td>54.650721</td>\n",
       "      <td>0.568</td>\n",
       "      <td>139.604632</td>\n",
       "      <td>0.509</td>\n",
       "      <td>58.871903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BERT_RET_GLOVE</td>\n",
       "      <td>68.911626</td>\n",
       "      <td>50.871143</td>\n",
       "      <td>0.799</td>\n",
       "      <td>172.246554</td>\n",
       "      <td>0.798</td>\n",
       "      <td>83.710332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SAL_RET_GLOVE</td>\n",
       "      <td>84.860473</td>\n",
       "      <td>55.409385</td>\n",
       "      <td>0.523</td>\n",
       "      <td>138.399581</td>\n",
       "      <td>0.472</td>\n",
       "      <td>60.462543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BERT_RET_RANDOM</td>\n",
       "      <td>68.859406</td>\n",
       "      <td>50.499527</td>\n",
       "      <td>0.754</td>\n",
       "      <td>158.334376</td>\n",
       "      <td>0.809</td>\n",
       "      <td>94.156765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SAL_RET_RANDOM</td>\n",
       "      <td>83.857680</td>\n",
       "      <td>54.183166</td>\n",
       "      <td>0.512</td>\n",
       "      <td>129.102266</td>\n",
       "      <td>0.463</td>\n",
       "      <td>49.903708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RET_ONLY_USE</td>\n",
       "      <td>31.907973</td>\n",
       "      <td>28.925607</td>\n",
       "      <td>0.850</td>\n",
       "      <td>8.818724</td>\n",
       "      <td>0.921</td>\n",
       "      <td>19.127585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RET_ONLY_TFIDF</td>\n",
       "      <td>34.919475</td>\n",
       "      <td>30.333290</td>\n",
       "      <td>0.949</td>\n",
       "      <td>11.397305</td>\n",
       "      <td>0.969</td>\n",
       "      <td>17.408584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RET_ONLY_GLOVE</td>\n",
       "      <td>16.050282</td>\n",
       "      <td>15.938262</td>\n",
       "      <td>0.975</td>\n",
       "      <td>11.323032</td>\n",
       "      <td>0.916</td>\n",
       "      <td>70.957015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RET_ONLY_RANDOM</td>\n",
       "      <td>12.527323</td>\n",
       "      <td>12.742683</td>\n",
       "      <td>0.979</td>\n",
       "      <td>9.160214</td>\n",
       "      <td>0.991</td>\n",
       "      <td>19.396728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BERT_STS_RET</td>\n",
       "      <td>68.947214</td>\n",
       "      <td>48.065070</td>\n",
       "      <td>0.763</td>\n",
       "      <td>162.269336</td>\n",
       "      <td>0.811</td>\n",
       "      <td>89.455739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>BACK_TRANS</td>\n",
       "      <td>25.863342</td>\n",
       "      <td>24.054174</td>\n",
       "      <td>0.955</td>\n",
       "      <td>35.006108</td>\n",
       "      <td>0.947</td>\n",
       "      <td>28.463902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>HUMAN</td>\n",
       "      <td>58.147799</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.672</td>\n",
       "      <td>3179.212349</td>\n",
       "      <td>0.799</td>\n",
       "      <td>67.244835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  BLEU_source  BLEU_human  fasttext_classifier  \\\n",
       "0              Source   100.000000   58.047103                0.045   \n",
       "1        CROSSALIGNED    47.952669   35.463996                0.768   \n",
       "2      STYLEEMBEDDING    77.977495   47.152522                0.095   \n",
       "3        MULTIDECODER    57.260200   39.086773                0.506   \n",
       "4        RETRIEVEONLY    25.861194   20.397801                0.921   \n",
       "5       TEMPLATEBASED    41.901829   29.100578                0.500   \n",
       "6          DELETEONLY    56.681112   38.387187                0.859   \n",
       "7   DELETEANDRETRIEVE    57.980202   39.437892                0.878   \n",
       "8            BERT_DEL    71.029978   51.991889                0.851   \n",
       "9             SEL_DEL    88.723662   57.264240                0.435   \n",
       "10       BERT_RET_USE    70.435568   49.499437                0.581   \n",
       "11        SAL_RET_USE    79.613983   50.704768                0.376   \n",
       "12     BERT_RET_TFIDF    70.601501   51.977224                0.781   \n",
       "13      SAL_RET_TFIDF    84.258325   54.650721                0.568   \n",
       "14     BERT_RET_GLOVE    68.911626   50.871143                0.799   \n",
       "15      SAL_RET_GLOVE    84.860473   55.409385                0.523   \n",
       "16    BERT_RET_RANDOM    68.859406   50.499527                0.754   \n",
       "17     SAL_RET_RANDOM    83.857680   54.183166                0.512   \n",
       "18       RET_ONLY_USE    31.907973   28.925607                0.850   \n",
       "19     RET_ONLY_TFIDF    34.919475   30.333290                0.949   \n",
       "20     RET_ONLY_GLOVE    16.050282   15.938262                0.975   \n",
       "21    RET_ONLY_RANDOM    12.527323   12.742683                0.979   \n",
       "22       BERT_STS_RET    68.947214   48.065070                0.763   \n",
       "23         BACK_TRANS    25.863342   24.054174                0.955   \n",
       "24              HUMAN    58.147799  100.000000                0.672   \n",
       "\n",
       "        klm_ppl  BERT_classifier    gpt2_ppl  \n",
       "0     73.058247            0.013   23.964528  \n",
       "1     70.702214            0.731   72.781838  \n",
       "2    105.763738            0.082  115.884521  \n",
       "3    159.232718            0.471  205.589666  \n",
       "4      8.249220            0.963   38.808384  \n",
       "5    246.939357            0.500  117.381526  \n",
       "6    100.170720            0.855   75.820355  \n",
       "7    118.709406            0.894   90.001013  \n",
       "8     97.584880            0.889   38.630072  \n",
       "9    100.755725            0.399   31.205129  \n",
       "10   149.433752            0.622   71.304381  \n",
       "11   174.985354            0.349   57.973565  \n",
       "12   129.279302            0.811   64.415130  \n",
       "13   139.604632            0.509   58.871903  \n",
       "14   172.246554            0.798   83.710332  \n",
       "15   138.399581            0.472   60.462543  \n",
       "16   158.334376            0.809   94.156765  \n",
       "17   129.102266            0.463   49.903708  \n",
       "18     8.818724            0.921   19.127585  \n",
       "19    11.397305            0.969   17.408584  \n",
       "20    11.323032            0.916   70.957015  \n",
       "21     9.160214            0.991   19.396728  \n",
       "22   162.269336            0.811   89.455739  \n",
       "23    35.006108            0.947   28.463902  \n",
       "24  3179.212349            0.799   67.244835  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res0.to_csv('matrics/yelp/matrics_yelp_all_model_prediction_0.csv')\n",
    "df_res1.to_csv('matrics/yelp/matrics_yelp_all_model_prediction_1.csv')\n",
    "df_resavg.to_csv('matrics/yelp/matrics_yelp_all_model_prediction_avg.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
